Timeline
--------
Start date: 11/1/25
Soft end date: ??? I think ~3-4 months is about right for the coding portion, potentially followed by 0-many months of compilation/editing if the book idea seems feasible. Could take a break in between those two phases if I like. I do think I should probably cap the experiment-running to a month or so, so I should aim to get to the point where I can start running experiments by ~end of dec or end of january. Added a couple reminders at the 2 month and 4 month marks to check in and make sure I'm not getting bogged down in any phase (unless I want to).

Output
------
Required:
- train >=1 chat model e2e: go from random weights -> base model -> chat model
- post-train (could be instruct tune, RL, other) >=1 larger model on custom data of some sort (basically, worried the $100 base model won't be good enough for my book idea; larger model will also presumably introduce new challenges which would be good to learn about)
- record multiple model diary entries (or letters to future/past selves, self portraits, poems, etc) throughout every phase of training

Probably should have:
- book (leaving this as "should have" on the off chance the writing quality just isn't there or this isn't coming together in an interesting way for whatever reason; but I do *really really* want to get this working)
    > seriously consider doing my "huggingface model as book" "publication"
- >=3 custom post training datasets and corresponding post training runs, e.g.
    - humor: try to train a really funny model
    - some kind of cognitive aging/death: train the model to be a *worse* reasoner, or even degrade back in the direction of random weights
    - shadow self or waluigi: mental illness, 4chan, train *against* spiral bench, aita take the unpopular position
    - eq gigabrain: n'th order theory of mind, ...?
    - writing quality: <3 eyeball kicks, don't @ me
    - just something very different from current llm personalities: e.g. extreme conviction vs constantly changing its opinion to match yours; being weird or shocking vs bland and inoffensive
    - some kind of rlvr or process-based RL? (not sure what this data would look like yet)
    - some kind of constitutional AI?
    - >=1 novel RL variant (don't worry about the end model being good, just hoping to spark some curiosity and encourage me to dive deeper into RL)
- blog post/website/etc to share
    > should try to share with karpathy, maybe some of the cyborgists, etc
- contribute some kind of pr(s) to nanochat
    > planning this feels weird but probably I will encounter things that could be fixed/improved/added and at least presenting the option to merge this is a nice thing for the community

Might do:
- discord where all the models continue interacting indefinitely (?) - cool but potentially expensive. Could do this for a short time to mine material for the book if things look promising. Could potentially try to get cyborgists to share infra, though I suspect that won't work.
- some kind of minimal streamlit app
    > just lots of fun building, and can imagine plenty of interesting variants will crop up as I go: completions from n different model "ages" in parallel to the same prompt; new convo btwn multiple model selves; another option for publishing the "book"

Concepts
--------
-large-scale llm training technical details
-RL/post-training
    > just sort of get a feel for this. Seems like the type of thing I could plausibly be good at.

Tech
----
-might make me brush up on torch a little? Probably mostly just troubleshooting installations and stuff tbh :/
-gpus (similar to above, probably mostly a matter of troubleshooting, but that's somewhat valuable too. If there is reason to dig in more, this is a good op to better understand different types tho - like if I end up needing to use different machines on different compute providers, it might help me understand tradoffs)

Learnings from lessons from last time
-------------------------------------
-I want to spend my Fun Side Project Time coding (or doing whatever it is I want to do, in some future iteration that could be writing), unblocked and in a way that allows for effortless flow. Work has all the docs, stakeholders, blockers, reviews, signoffs, whatever. This should just be a chance for my mind and fingers to run wild. In other words, doing a course instead of building something on my own was kind of a buzzkill. Spent a lot of time digging through the book or trying to figure out how to reproduce the exact semicolon placement in my error messages that the course tests required when I wanted to just let the code flow. That's not to say I should never do a course again but I should proceed with caution when making that choice.
    > I do suspect there will be a fair bit of IT troubleshooting this time, but maybe can get claude code/goose to fix a lot of that :D. I am not doing a course which is good. I'll be using a lot of karpathy's pre-built functionality which I could see being good or bad - on one hand, in theory this frees me up to write just the bits of code *I* care about. It could also make the project unsatisfying, perhaps in a similar way to agentic coding. But I'm not super worried about this, I think there's plenty of interesting code for me to write here and interesting thinking to do: constructing datasets, writing callbacks, etc. And I shouldn't be blocked by needing to read lots of requirements or whatever. Importantly, my imagination can guide me: if I get excited about an idea, I can just go do it. I think this was one really valuable aspect of my earlier side projects, it sort of forced me to develop some level of comfort with open ended problems and at least be able to fake, uh, agenticness? (Volition, probably.) The codecrafters course was very much "implementing the spec" which I don't think can really capture my imagination in the same way.

-going in with low expectations was nice. I feel no pressure to share this anywhere and have no concerns around whether it will get used. Of course, maybe that's the wrong lesson: "adoption is hard so don't build useful things". But I think it's fair to say not *every* project needs to be that, and it's worth thinking carefully about what sort of project you want to build next.
    > whelp, I didn't really follow this one. I do have high-ish hopes here. Not that any of the resulting models will be particular brilliant per se, I certainly don't expect SOTA performance or anything remotely close on any existing benchmark. But for the book, I suppose. I do think I'd be pretty happy if it comes out well even if no one else reads it though - obviously it would be cool if karpathy or cyborgists did but I don't expect that.

-seeing the end product definitely felt less satisfying than when it was something summoned from my own imagination. Again, course vs custom.
    > somewhat addressed here? Not doing a course anyway...Not sure how "mine" the model will feel, I guess that depends on how dramatically it changes (if I posttrain a giant existing model) or how far I deviate from karpathy's default nanochat setup. The book would be very much mine though and should be as rewarding as that implies.

-I've really made Finishing Things a core part of this side project series so it's crucial to choose wisely upfront. I'm going to have a really hard time talking myself into allowing quitting, even when it might make sense. Maybe I can design the project upfront with multiple offramps throughout. If things are going well, I can continue. But if for any reason it makes sense to exit early, I can do that without guilt.
    > I think this does have a relatively early off ramp if I do want to do that for whatever reason. Train one standard nanochat model e2e, maybe 1-2 custom posttraining runs, and if that doesn't look promising can always call an audible and end it there.

-I'm again reminded of how true parrt's words are about every problem either fostering or eating away at your confidence. There were definitely times where I was truly worried that the computer might finally win and I would have to live with that defeat. But that should make my eventual victory all the more informative: even a roadblock that big was not enough to bring computer victory.
    > tbh most worried about IT issues here. Pray I can get claude code working here and it just handles a lot of that...

What could go wrong?
--------------------
- excessive IT issues around compute setup
    > pls no, but 1) maybe claude code can fix things here (and in turn incentivize me to get better with it) and 2) learning this could be somewhat valuable.
- feels too much like work (esp if using workstations ðŸ‘€)
    > legit risk...but worth it if it provides the compute
- doesn't feel rewarding due to using too much karpathy functionality
    > if the book comes together I don't think that will be too big of a concern. And the custom datasets, just pour a lot of effort into that I guess? If it really comes down to it I can try to improve some portion of the codebase and PR it I guess, would rather do that than try to rewrite his existing work.
- small models just can't write well enough to generate compelling writing for my book idea
    > quite possible...prepare for that disappointment. postraining a 20b model or so on deepseek writing might get us there though - would limit scope of book to posttraining, which is a bit unfortunate, but still cool. I can also view this as experience to help inform a potential future me-written version of this book.
- compute expenses force me to choose between spending an insane amount of $ here OR stopping at relatively uninteresting results
    > pls no. I guess I do have 2-3 promising options here before I have to make that choice so hopefully it won't come to that.
- posttraining turns out to be just kinda boring? (if I just posttrain on a bunch of good examples and the model complies with that, it would be kinda boring?)
    > legit possibility, but would be good to learn if so. And I think 1) I can really make an effort here to construct interesting tasks, create the right conditions for an interesting mind to "grow" vs imposing the exact behavior I want, 2) my impression is RL can easily go off the rails in weird/frustrating ways, which is often bad but here might be sort of good? A junkyard full of weird or unhinged RL mutants would be super interesting.
- run out of space for all the model I want to save
    > assuming (ðŸ˜¬) huggingface upload works, I should be able to save quite a few there even if I can't keep them all around locally at once. And if it doesn't work, I have bigger problems.
